enabled = 1
mode = absolute
threshold = 0.9

ip_whitelist = [
    198.110.85.*
]

plugins.UserAgent = {
    banned_useragents = [
         "MJ12bot", "AhrefsBot" , "GoogleBot", "SemrushBot", "opensiteexplorer", "DotBot", "UCBrowser","serpstatbot", "LieBaoFast", 
         "Bytespider", "zh-CN", "splash", "Bing", "DotBot", "Yandex", "BLEXBot", "PetalBot", "Barkrowler", "webmeup", "dotbot", 
         "bot", "crawl","spider"
    ],
        
     # Ban user agent on dynamically loaded list?
    use_useragent_list = 1,
    
    # URL for user agent ban list. List is JSON in the format defined at https://github.com/monperrus/crawler-user-agents
    useragent_list_url = "https://raw.githubusercontent.com/monperrus/crawler-user-agents/refs/heads/master/crawler-user-agents.json",
    
    # Time in seconds between refreshed of user agent ban list
    useragent_list_ttl = 21600,
    
    # List of user agent patterns to always pass (eg. exclude from useragent list)
    exclude_useragents = ["google"],
    
    # Force reload of user agent list regardless of ttl
    useragent_list_force_reload = 0
}



plugins.RequestFrequency = {
    # Maximum number of page loads per second allows before we ban
    frequency_threshold = 150,

    # Ban weight to return (1.0 = absolutely ban)
    ban_probability = 0,
}

plugins.IPAddress = {
    # List of ip addresses to ban
    # You may use wildcards to match ranges
    # Eg. 10.55.*.*
    banned_ip_addresses = [
        10.44.2.*,216.244.66.*,184.175.128.*,74.204.25.*, 64.124.8.*
    ]
}
